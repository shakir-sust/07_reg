---
title: "Regression"
format: html
---

# Learning objectives  
Today's learning objectives are to:  
-   Explore a data set containing corn grain yield response to seeding rate coming from a *well-conducted randomized complete-block design*.

- Complete formal analysis of four models:
  - intercept-only  
  - linear (intercept + slope)  
  - quadratic  
  - linear-plateau  

- Use bootstrap to create confidence intervals around regression lines.  
  
-   Compare all models using **AIC**. Which one fits the data best? Choose one to proceed and use in the next steps.

-   Use regression for finding level of input (seeds/ha) that optimize crop output (yield).

# Introduction  
# Regression use - Finding optimum input level

One of the main goals of applying different levels of an input (e.g., seeding rate) and measuring its effect on an output (e.g., yield) is to estimate the **optimum input level that maximizes the output**.

Here, our input is seeding rate, but it could be a range of other types of inputs:  
  - Fertilizer  
  - Pesticide  
  - Irrigation volume  
  - Temperature and air relative humidity (controlled environments)   
  - Planting date  
  - Others?

Because both the response variable (i.e., corn yield) and explanatory variable (i.e., seeding rate) are **numerical**, we can analyze this in a **regression** approach (instead of ANOVA).  

## Different input x output responses

Anytime we have this input x output **numerical** relationship, a few different patterns can emerge.

```{r input output relationships figure, echo=F}
#knitr::include_graphics("../data/ior.png")
```

Talk about each of these patterns.

# 1) Setup  

Here is where we load the packages we will use.

```{r setup}
#| message: false
#| warning: false

#install.packages("broom.mixed")
#install.packages("nlraa")
#install.packages("lmerTest")
#install.packages("nlme")
#install.packages("metrica")
#install.packages("knitr")

# Loading packages
library(tidyverse) # for data wrangling and plotting
library(janitor) # clean column names
library(lmerTest) # for mixed-effect modeling
library(broom.mixed) # for residual diagnostics
library(knitr) # for figure displaying
library(nlme) # for non-linear modeling
library(car)
library(nlraa) # for starting value functions
library(metrica) # for rmse

```

Reading data and doing some light wrangling.  
```{r}
#| message: false
reg_dfw <- read_csv("../data/reg.csv") %>%
  clean_names() %>% #to standardize the variable names in terms of lower case and upper case
  mutate(rep = factor(rep))

reg_dfw
```

This study was a randomized complete block design (RCBD) with four blocks.  

The treatment factor is seeding rate (in 1,000 seeds per ha) with five levels:  
  - 40  
  - 60  
  - 80  
  - 100  
  - 120  

The response variable was corn yield in Mg/ha.  


# 2) EDA  
```{r summary}
summary(reg_dfw)
```
Yield ranging from 7.8 to 15.6 Mg/ha.  

```{r reg exp boxplot}
ggplot(data = reg_dfw,
       aes(x = factor(sr_ksha), #factor(sr_ksha) is to treat 
           y = yield_mgha
           )) +
  geom_boxplot() 

```

What is going on with this boxplot?

```{r reg plot point + smooth, message=FALSE, warning=FALSE}
ggplot(reg_dfw, 
       aes(x = sr_ksha, 
           y = yield_mgha)) +
  geom_point() + #To plot the data points
  geom_smooth() #To add trendline #We use geom_line() only to connect the points with a line
  

```

Let's fit 4 different models to assess which one fits the data the best. 

Our goal is to then use that model to estimate the optimum seeding rate for this study.  

# 3) Intercept-only 
## a) Model  
```{r mod1_int}
# Changing to sum-to-zero contrast
options(contrasts = c("contr.sum", "contr.poly"))

# Model fitting
mod1_int <- lmer(yield_mgha ~ (1|rep),
                 data = reg_dfw
                 ) #lmer is for either random effect of mixed efect model

# Summary
summary(mod1_int)

```

## b) Model Assumptions
```{r mod1 augmenting}
# Augmenting and adding perason standardized residuals
mod1_int_aug <- augment(mod1_int) %>%
  mutate(.stdresid = resid(mod1_int, 
                           type = "pearson", #to get pearson residual
                           scaled = T)) %>% #to get normalized i.e., scaled residual to detect outliers
  left_join(reg_dfw) #to bring back the seeding rate (x) into the model


mod1_int_aug
```

### Within-group errors are iid ~ N(0, var2)

```{r mod1 Standardized Residuals vs. Fitted, message=FALSE, warning=FALSE}
ggplot(mod1_int_aug, 
       aes(x = .fitted, 
           y = .stdresid))+
  geom_hline(yintercept = 0, 
             color = "red")+ #To add a horizontal line across 0
  geom_point(size = 3, 
             alpha = .7)+
  geom_smooth()
```

Residuals looking suspicious. There is an increasing pattern.

For now, let's keep going.

```{r mod1 Quantile-Quantile}
ggplot(mod1_int_aug, 
       aes(sample = .stdresid))+
  stat_qq()+
  stat_qq_line()
```
Some deviations at the tails, not too bad.  

### Random effects are iid ~ N(0,var1)

On this plot, looking for normality.

```{r mod1 QQ plot for Location:fRep random effects}
mod1_int_randeff <- ranef(mod1_int)[[1]] 

ggplot(mod1_int_randeff, 
       aes(sample = `(Intercept)`))+
  stat_qq()+
  stat_qq_line()

```
Few observations, nothing alarming.  

## c) Model summary

```{r mod1 ANOVA}
summary(mod1_int)
```

Intercept highly significant! Which does not mean that this is a good/bad model.

## d) Final plot  

```{r mod1 final plot}
ggplot(mod1_int_aug, 
       aes(x = sr_ksha, 
           y = yield_mgha))+
  geom_point(size = 3, 
             alpha = .7) +
  geom_line(aes(y = .fixed))
```

Problem with the plot above:  
- no confidence interval around regression

Solution:  
- we can use bootstrap to create confidence intervals around the regression curve

Bootstrap: Resampling with replacement

First, let's create an data set with all levels of seeding rate we want to get a prediction.  
```{r nd}
nd <- data.frame(sr_ksha = seq(40, 120, 1)) #nd stands for new data #40 is the lowest level, 120 is the hightest level, and we want the increment to happen at a step of 1

nd
```

```{r mod1 better final plot}
# Creating predictions
nd <- nd %>%  #The model is gonna predict based on intercepts
  mutate(mod1_yield_mgha = predict(mod1_int,
                                   nd,
                                   re.form = NA
                                   ))
# Creating function to bootstrap
predict.fun <- function(mod) {
  predict(mod, 
          newdata = nd, 
          re.form = NA)
}

# Bootstrapping for confidence interval
mod1_int_boots <- bootMer(mod1_int, 
                          predict.fun, 
                          nsim = 200) %>% #bootstrapping 200 times
  confint() %>%
  as.data.frame() %>%
  rename(mod1_int_lcl = `2.5 %`,
         mod1_int_upl = `97.5 %`)

mod1_int_boots

nd <- nd %>%
  bind_cols(mod1_int_boots)

nd

# Final plot
ggplot(reg_dfw, 
       aes(x = sr_ksha, 
           y = yield_mgha))+
  geom_point(size = 3, 
             alpha = .7)+
  geom_line(data = nd, 
            aes(y = mod1_yield_mgha), 
            color = "forestgreen") +
  geom_ribbon(data = nd,
              aes(ymin = mod1_int_lcl,
                  ymax = mod1_int_upl,
                  x = sr_ksha
                  ),
              inherit.aes = FALSE,
              alpha = .5
              )
```

Linear thoughts:

Just because p-value (of the intercept) is (highly) significant, it DOES NOT mean the (regression) model is good. Always check residuals and plot!!

Next, let's try a linear (intercept + slope) model.  

# 4) Linear regression  

## a) Model  
```{r mod2 linear model}
# Changing to sum-to-zero contrast
options(contrasts = c("contr.sum", "contr.poly"))

# Model fitting
mod2_lin <- lmer(yield_mgha ~ (1|rep) + sr_ksha, #"(1|rep)" is to set rep as random effect
                 data = reg_dfw
                 )

# Summary
summary(mod2_lin)

```

## b) Model Assumptions
```{r mod2 augmenting}
# Augmenting and adding perason standardized residuals
mod2_lin_aug <- augment(mod2_lin) %>%
  mutate(.stdresid = resid(mod2_lin, 
                           type = "pearson", 
                           scaled = T))


mod2_lin_aug
```

### Within-group errors are iid ~ N(0, var2)

```{r mod2 Standardized Residuals vs. Fitted}
ggplot(mod2_lin_aug, 
       aes(x = .fitted, 
           y = .stdresid))+
  geom_hline(yintercept = 0, color = "red")+
  geom_point(size = 3, alpha = .7)+
  geom_smooth()
```

Residuals looking suspicious! Clear quadratic pattern! We will need to address this problem later.

For now, let's keep going.

```{r mod2 Quantile-Quantile}
ggplot(mod2_lin_aug, 
       aes(sample = .stdresid))+
  stat_qq()+
  stat_qq_line()
```
Tails looking a bit off now.  

### Random effects are iid ~ N(0,var1)

On this plot, looking for normality.

```{r mod2 QQ plot for Location:fRep random effects}
mod2_lin_randeff <- ranef(mod2_lin)[[1]] 

ggplot(mod2_lin_randeff, 
       aes(sample = `(Intercept)`))+
  stat_qq()+
  stat_qq_line()

```
Few points, not too bad.  

## c) Model summary

```{r mod2 ANOVA}
summary(mod2_lin)
```

Intercept and slope for sr_ksha are highly significant!

## d) Final plot  

```{r mod2 final plot}
ggplot(mod2_lin_aug, 
       aes(x = sr_ksha, 
           y = yield_mgha))+
  geom_point(size = 3, 
             alpha = .7) +
  geom_line(aes(y = .fixed)) +
  scale_x_continuous(limits = c(0, 120))
```

Problem with the plot above:  
- no confidence interval around regression

Solution:  
- we can use bootstrap to create confidence intervals around the regression curve

```{r mod2 better final plot}
# Creating predictions
nd <- nd %>%
  mutate(mod2_yield_mgha = predict(mod2_lin, 
                                   nd, 
                                   re.form = NA))

# Bootstrapping for confidence interval
mod2_lin_boots <- bootMer(mod2_lin, 
                          predict.fun, 
                          nsim = 200) %>%
  confint() %>%
  as.data.frame() %>%
  rename(mod2_lin_lcl = `2.5 %`,
         mod2_lin_upl = `97.5 %`)


nd <- nd %>%
  bind_cols(mod2_lin_boots) #To bind the columns

nd

# Final plot
ggplot(reg_dfw, 
       aes(x = sr_ksha, 
           y = yield_mgha))+
  geom_point(size = 3, 
             alpha = .7)+
  geom_line(data = nd, 
            aes(y = mod2_yield_mgha), 
            color = "forestgreen")+
  geom_ribbon(data = nd, 
              aes(x = sr_ksha, 
                  ymin = mod2_lin_lcl,
                  ymax = mod2_lin_upl),
              fill = "gray", 
              alpha = 0.5, 
              inherit.aes = FALSE)
```

# 5) Quadratic regression  
## a) Model

```{r mod3 model}
# Changing to sum-to-zero contrast
options(contrasts = c("contr.sum", "contr.poly"))

# Model fitting
mod3_quad <- lmer(yield_mgha ~ (1|rep) + sr_ksha + I(sr_ksha^2), #We are using lmer() because we are treating blocks as random effect i.e., to run a mixed effects model #I() stands for identity, used to make sure that R understands it is a quadratic term #rep is categorical, and sr_ksha is numeric variable
                  data = reg_dfw
                  )

# Summary
summary(mod3_quad)
```
Here, the "Estimate" for "sr_ksha" is 0.3089371, which is greater than 0. This tells us that there is a positive relationship between seeding rate and yield (based on the slope estimate of the model).

The "Estimate" for the quadratic term "I(sr_ksha^2)" is -0.0016061 which is a negative value -- which indicates that there is a point of maximum in the (upward) quadratic curvature. If the "Estimate" for the quadratic term "I(sr_ksha^2)" was positive, it would mean that we have a point of minimum  in the (downward) quadratic curvature.

## Differnce between regression and ANOVA:

In regression, the explanatory variable(s) (= x) are numerical
In ANOVA, the explanatory variable(s) (= x) are categorical


## b) Model Assumptions
```{r mod3 augmenting}
# Augmenting and adding pearson standardized residuals
mod3_quad_aug <- augment(mod3_quad) %>% #"augment()" function allows to collect all the residual information (along with our original dataframe)
  mutate(.stdresid = resid(mod3_quad, 
                           type = "pearson", 
                           scaled = T))


mod3_quad_aug
```

### Within-group errors are iid ~ N(0, var2)

```{r mod3 Standardized Residuals vs. Fitted}
ggplot(mod3_quad_aug, 
       aes(x = .fitted, 
           y = .stdresid)) + #x = fitted, y = residual #fitted by residual plot
  geom_hline(yintercept = 0, color = "red")+
  geom_point(size = 3, alpha = .7)+
  geom_smooth(method = "lm")
```
Very important note: 
Question: Why do we check assumptions of residuals, and not on the raw data?
Answer: The reason we chech assumptions on residuals in because residuals change based on the model, whereas raw data does not change. When we change the model, we also change the residuals. That's why we chech assumptions on residuals. The residual in the interaction between our data and model. So, if we change the model, the residuals are also going to change. 

Dr. Bastos said: make the model fit your data, not the data fit your model (first).



Residuals are looking better now, no pattern.  

Linear thoughts:

Model assumptions are based on residuals, not raw data!

Notice here that we used the **same data** as before, just changed the model, and that completely changed the residuals (for better, in this case)!

Remember: residual = distance of raw data from model fit [Residual, e = observed data (y_i) - predicted data (y_hat)]. If model changes, residual changes, even when same underlying raw data is used.


```{r mod3 Quantile-Quantile}
ggplot(mod3_quad_aug, 
       aes(sample = .stdresid))+
  stat_qq()+
  stat_qq_line()
```
Looking better than before, especially tails.  

### Random effects are iid ~ N(0,var1)

On this plot, looking for normality.

```{r mod3 QQ plot for Rep random effect}
mod3_quad_randeff <- ranef(mod3_quad)[[1]] 

ggplot(mod3_quad_randeff, 
       aes(sample = `(Intercept)`))+
  stat_qq()+
  stat_qq_line()

```
Looks ok.  

## c) Model summary

```{r mod3 ANOVA}
summary(mod3_quad)

```

Slope and curvature for sr_ksha are highly significant!

## d) Final plot

```{r mod3 final plot}
ggplot(mod3_quad_aug, 
       aes(x = sr_ksha, 
           y = yield_mgha))+
  geom_point(size = 3, alpha = .7)+
  geom_line(aes(y = .fixed), 
            color = "forestgreen")

```

Problems with the plot:   
- regression curve on the plot above is not continuous because it is based on our original levels of SR (40, 60, 80, 100, 120 k seeds/ha).

-   similar to linear regression, no confidence interval.

Solutions:   
- to create a smoother look, we can simulate some SR data, use the model above to predict their yield, and plot that as a line.

-   we can use bootstrap to create confidence intervals around the regression curve

```{r mod3 better final plot}
# Creating predictions
nd <- nd %>%
  mutate(mod3_yield_mgha = predict(mod3_quad, 
                                   nd, 
                                   re.form = NA))

# Bootstrapping
mod3_quad_boots <- bootMer(mod3_quad, 
                           predict.fun, 
                           nsim = 200) %>%
  confint() %>%
  as.data.frame() %>%
  rename(mod3_quad_lcl = `2.5 %`,
         mod3_quad_upl = `97.5 %`)


nd <- nd %>%
  bind_cols(mod3_quad_boots)

nd

# Final plot
ggplot(reg_dfw, 
       aes(x = sr_ksha, 
           y = yield_mgha))+
  geom_point(size = 3, alpha = .7)+
  geom_line(data = nd, 
            aes(y = mod3_yield_mgha), 
            color = "forestgreen")+
  geom_ribbon(data = nd, aes(x = sr_ksha, 
                             ymin = mod3_quad_lcl,
                             ymax = mod3_quad_upl),
              fill = "gray", 
              alpha = 0.5, 
              inherit.aes = FALSE)



```

Now, which one fits the data best?
Which one should we chose for finding the optimum, or predicting new data?  
Answer: Quadratic model

In our data set,   
- We know there is a yield response to SR (so intercept-only model is not a good candidate),  
- We know we have achieved a maximum point (so linear is not a good candidate)  
- We have already fit the quadratic model. (i.e., there is a point of maximum)  
- We can fit the linear-plateau (LP) model. (Linear-plateau (LP) is the first non-linear model that we are gonna run in this class)

So, let's fit a LP model and then compare it to the quadratic.  
After that, we can choose the model that best fit our data and use it to extract the optimum seeding rate.

# 6) Linear-plateau regression  
## a) Model

```{r mod4 model}
# Changing to sum-to-zero contrast
options(contrasts = c("contr.sum", "contr.poly"))

reg_dfw
# Model fitting

mod4_linp <- nlme(yield_mgha ~ SSlinp(sr_ksha, a, b, xs),
                  data = reg_dfw,
                  random = list(rep = pdDiag(a + b + xs ~ 1)),
                  fixed = list(a ~ 1,
                               b ~ 1,
                               xs ~ 1),
                  start = c(a = 0, 
                            b = 0.3,
                            xs = 100))

#nlme() - stands for "non linear mixed effect" #a, b, xs are non-linear parameters: a = intercept, b = slope, xs = break point of the plateau

# Summary
summary(mod4_linp)
```

## b) Model Assumptions
```{r mod4 augmenting}
# Augmenting and adding pearson standardized residuals
mod4_linp_aug <- augment(mod4_linp,
                         data = reg_dfw) %>%
  mutate(.stdresid = resid(mod4_linp, 
                           type = "pearson", 
                           scaled = T))


mod4_linp_aug
```

### Within-group errors are iid ~ N(0, var2)

```{r mod4 Standardized Residuals vs. Fitted}
ggplot(mod4_linp_aug, 
       aes(x = .fitted, 
           y = .stdresid))+
  geom_hline(yintercept = 0, color = "red")+
  geom_point(size = 3, alpha = .7)+
  geom_smooth(method = "lm")
```
Looking good.  

```{r mod4 Quantile-Quantile}
ggplot(mod4_linp_aug, 
       aes(sample = .stdresid))+
  stat_qq()+
  stat_qq_line()
```
Looking good.  

### Random effects are iid ~ N(0,var1)

On this plot, looking for normality.

```{r mod4 QQ plot for Rep random effect}
mod4_linp_randeff 

ggplot(mod4_linp_randeff, 
       aes(sample = estimate))+
  stat_qq()+
  stat_qq_line() 

```
b and xs random estimates are so small that seem to be all zero.  
That's not a problem per se, just a fact around their variability.  

## c) Model summary
```{r mod4 ANOVA}
summary(mod4_linp)

```

a, b, and xs are highly significant!

## d) Final plot

```{r mod4 final plot}
ggplot(mod4_linp_aug, 
       aes(x = sr_ksha, 
           y = yield_mgha))+
  geom_point(size = 3, alpha = .7)+
  geom_line(aes(y = .fixed), 
            color = "forestgreen")

```

Problems with the plot:   
- regression curve on the plot above is not continuous because it is based on our original levels of SR (40, 60, 80, 100, 120 k seeds/ha).

-   similar to linear regression, no confidence interval.

Solutions:   
- to create a smoother look, we can simulate some SR data, use the model above to predict their yield, and plot that as a line.

-   we can use bootstrap to create confidence intervals around the regression curve

```{r mod4 better final plot}
# Creating predictions
nd <- nd %>%
  mutate(mod4_yield_mgha = predict(mod4_linp, 
                                   nd, 
                                   level = 0))  

# Non-linear prediction function  
predict.fun.nl <- function(x) predict(x,
                           newdata = nd,
                           re.form = NA,
                           level = 0)


# Bootstrapping
mod4_linp_boots <- boot_nlme(mod4_linp, 
                             f = predict.fun.nl, 
                             R = 200) %>%
  confint() %>%
  as.data.frame() %>%
  rename(mod4_linp_lcl = `2.5 %`,
         mod4_linp_upl = `97.5 %`)

nd <- nd %>%
  bind_cols(mod4_linp_boots)

nd

# Final plot
ggplot(reg_dfw, 
       aes(x = sr_ksha, 
           y = yield_mgha))+
  geom_point(size = 3, alpha = .7)+
  geom_line(data = nd, 
            aes(y = mod4_yield_mgha), 
            color = "forestgreen")+
  geom_ribbon(data = nd, aes(x = sr_ksha, 
                             ymin = mod4_linp_lcl,
                             ymax = mod4_linp_upl),
              fill = "gray", 
              alpha = 0.5, 
              inherit.aes = FALSE)



```

# 7) Model comparison  
## a) Visual comparison
```{r comparison plot}
ggplot(reg_dfw, aes(x = sr_ksha, y = yield_mgha))+
  geom_point(size = 4, alpha = .6)


```
## Table comparison  
```{r}
IC_tab(mode1_int,
       mod2_lin,
       mod3_quad,
       mod4_linp
       )
```
Based on the above, model 4 (linear-plateau) had the lowest AIC and thus should be used to find the optimum level of seeding rate. The lower the AIC, the better. 

# 8) Optimum on best model  
Because our best model was the linear-plateau, let's find its seeding rate that optimized yield.  

```{r optimum SR}
mod4_linp %>%
  intervals(which = "fixed")

```
Based on the linear-plateau model, the level of seeding rate to optimize corn grain yield in this study was **73.47** thousand seeds/ha.  

Now let's predict what was the yield at that seeding rate.  
```{r yield at optimum SR}
predict(mod4_linp,
        newdata = data.frame(sr_ksha = 73.48),
        level = 0
        )

```
At the optimum seeding rate of **73.47** thousand seeds/ha, corn grain yield was **13.45** Mg/ha.

```{r final plot}
ggplot(reg_dfw, aes(x = sr_ksha, y = yield_mgha))+
  geom_point(size = 4, alpha = .6)+
  geom_line(data = nd, aes(y = mod4_yield_mgha), 
            color = "orange",
            size = 1.5) 

```
# 9) Take-home  

-   We use regression when both y and x are **numerical**  

-   Finding optimum: should run multiple models, see which one fits the data best, and choose that one to estimate optimum

-   Always check residuals! p-values alone do not tell you whether model is adequate for your data!



